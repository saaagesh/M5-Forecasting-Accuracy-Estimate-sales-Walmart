{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e61f99-1f91-4bda-8cdb-afc01001fef8",
   "metadata": {},
   "source": [
    "### Retail demand forecasting with Vertex Forecast and Pipelines\n",
    "\n",
    "This notebook demonstrates the use of Vertex Pipelines to orchestrate Vertex Forecast (VF) workflows. It makes use of three tables that define the retail demand schema for this example:\n",
    "\n",
    "**Dataset**\n",
    "* Uses the M5 public dataset\n",
    "\n",
    "\n",
    "**Tables in the dataset**\n",
    "- **activity**: activity table, containing time-variant information, e.g., sales, promo, etc.\n",
    "- **product**: the product table containing metadata of items\n",
    "- **plan table**: the table used to generate forecasts for future periods where actuals are unknown\n",
    "\n",
    "**Modeling approach**\n",
    "* Two models are trained, each with a different optimization objective: `minimize-rmse` & `minimize-mape`\n",
    "* The predictions from both models aere stacked (averaged), to produce the final prediction\n",
    "* In the training job configuration, `additional_experiments` is used to specify two overrides:\n",
    "> * `'forecasting_model_type_override': 'seq2seq'` - instructs VF to only train a sequence-to-sequence model\n",
    "> * `'forecasting_hierarchical_group_column_names':'dept_id, cat_id'` - specifies hierarchical modeling, and which input features to aggregate\n",
    "\n",
    "**Model Pipeline**\n",
    "* This pipeline includes all steps needed to train and evlauate the models, as well as generate a forecast/prediction for the future\n",
    "* It includes examples of custom pipeline components and pre-built components from Vertex AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a34ed-703a-4e57-823c-3b8c5b794966",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2251c2-06cb-4179-9a35-3d42fe2a27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'sk-ai-ml-poc'  # <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' \n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303623b-162e-4921-9263-6505d7927e32",
   "metadata": {},
   "source": [
    "### Install KFP SDK and Vertex Pipelines client library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718cf38-b836-43ae-84ac-da6df463cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "!git clone https://github.com/kubeflow/pipelines.git\n",
    "!pip install pipelines/components/google-cloud/.\n",
    "!pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518ddbb-40e7-4ff5-993f-9b38fa4c7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e0df1-f068-4069-9ae4-4f04b480e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65853a4-f5cc-4ce6-a338-d1e902c0422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Project Configuration:\n",
    "# project where pipeline and vertex jobs are executed\n",
    "\n",
    "PROJECT_ID = 'sk-ai-ml-poc' # {type: 'string'} <--- TODO: CHANGE THIS\n",
    "PROJECT_NUMBER = '555195908111' # {type: 'string'} <--- TODO: CHANGE THIS\n",
    "LOCATION = 'us-central1' # {type: 'string'} \n",
    "# BQ_LOCATION = 'US' # {type: 'string'}\n",
    "\n",
    "SCOPES = (\n",
    "  'https://www.googleapis.com/auth/cloud-platform',\n",
    ")\n",
    "\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'\n",
    "assert PROJECT_NUMBER, 'the value for this variable must be set'\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ba30f-c053-4311-b526-95e929cfd0f4",
   "metadata": {},
   "source": [
    "## Pipeline Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20aece2-0417-4f5e-ae9d-1808e50fe12d",
   "metadata": {},
   "source": [
    "### Set some variables\n",
    "\n",
    "**Before you run the next cell**, **edit it** to set variables for your project.  \n",
    " \n",
    "* For `BUCKET_NAME`, enter the name of a Cloud Storage (GCS) bucket in your project.  Don't include the `gs://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86725999-25a2-43b9-b56b-87c96444c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Pipeline Parameters\n",
    "USER = 'skewalramani'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "BUCKET_NAME = 'sk-forecasting'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "GS_PIPELINE_ROOT_PATH = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
    "\n",
    "\n",
    "print('GS_PIPELINE_ROOT_PATH: {}'.format(GS_PIPELINE_ROOT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6d826-ad8d-4dc0-b13f-c55ea06e41ac",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0374ec-2d7a-4bba-b353-496439818bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from google import auth\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "import google.cloud.aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "#from google.colab import auth as colab_auth\n",
    "#from google.colab import drive\n",
    "\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4baf5-fb67-4e9f-bb29-c7580ae87fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'aiplatform SDK version: {google.cloud.aiplatform.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b1034-d33a-487b-94bd-4318fe1af9c0",
   "metadata": {},
   "source": [
    "BQ and pipeline clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30998b18-7597-4ca7-948e-1bf0eb333474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab_auth.authenticate_user()\n",
    "credentials, _ = auth.default()\n",
    "credentials, _ = auth.default(scopes=SCOPES, quota_project_id=PROJECT_ID)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
    "pipeline_client = pipelines_client.AIPlatformClient(\n",
    "  project_id=PROJECT_ID,\n",
    "  region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700245e-e1e6-4018-bb45-4d6b1efb101c",
   "metadata": {},
   "source": [
    "# Training, Evaluation, and Forecast Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bdebe-0ce1-4f1f-ac15-57fa51e51e55",
   "metadata": {},
   "source": [
    "### Pipeline visualization in Vertex UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3365ed6-c4e5-4e24-ac3c-bb1c36cef270",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = 'gs://sk-forecasting/pipelines/pipelines.json'\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "  with open(PIPELINES_FILEPATH) as f:\n",
    "    PIPELINES = json.load(f)\n",
    "else:\n",
    "  PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "  with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "    json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de7f22-152e-4c19-8a1e-10285900f437",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def64c0f-a5b2-497c-ad21-c722ba643fb6",
   "metadata": {},
   "source": [
    "### Pipeline Component: **Create BQ dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eddd9b-e7d5-408f-9099-9145c6b17e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.18.0'],\n",
    ")\n",
    "def create_dataset(\n",
    "    project: str, \n",
    "    vertex_dataset: str, \n",
    "    eval_dataset: str, \n",
    "    bq_location: str):\n",
    "  from google.cloud import bigquery\n",
    "\n",
    "  bq_client = bigquery.Client(project=project, location=bq_location)\n",
    "  # (\n",
    "  #     bq_client.query(f'CREATE SCHEMA IF NOT EXISTS `{project}.{vertex_dataset}`')\n",
    "  #     .result()\n",
    "  # ),\n",
    "  (\n",
    "      bq_client.query(f'CREATE SCHEMA IF NOT EXISTS `{project}.{eval_dataset}`')\n",
    "      .result()\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f305d85-91ac-4b0b-b175-d12f5ca35b61",
   "metadata": {},
   "source": [
    "### Pipeline component: **Input Table Specs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559286d-277f-445a-a1e2-ac2b998df408",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def create_input_table_specs(\n",
    "  products_table_uri: str,\n",
    "  activities_table_uri: str,\n",
    "  locations_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    ") -> NamedTuple(\n",
    "  'Output',\n",
    "  [('input_table_specs', str), ('model_feature_columns', str)],\n",
    "):\n",
    "  import json\n",
    "\n",
    "  products_table_specs = {\n",
    "    'bigquery_uri': products_table_uri,\n",
    "    'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "    'forecasting_attribute_table_metadata': {\n",
    "      'primary_key_column': 'product_id'\n",
    "    }\n",
    "  }\n",
    "\n",
    "  locations_table_specs = {\n",
    "    'bigquery_uri': locations_table_uri,\n",
    "    'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "    'forecasting_attribute_table_metadata': {\n",
    "      'primary_key_column': 'location_id'\n",
    "    }\n",
    "  }\n",
    "\n",
    "  activities_table_specs = {\n",
    "    'bigquery_uri': activities_table_uri,\n",
    "    'table_type': 'FORECASTING_PRIMARY',\n",
    "    'forecasting_primary_table_metadata': {\n",
    "        'time_column': 'date',\n",
    "        'target_column': 'gross_quantity',\n",
    "        'time_series_identifier_columns': ['product_id', 'location_id'],\n",
    "        'unavailable_at_forecast_columns': [],\n",
    "        'time_granularity': {\n",
    "          'unit': time_granularity_unit,\n",
    "          'quantity': time_granularity_quantity,\n",
    "        },\n",
    "        # 'predefined_splits_column': 'ml_use',\n",
    "        # 'predefined_split_column': 'ml_use', # model_override\n",
    "    }\n",
    "  }\n",
    "\n",
    "  model_feature_columns = [\n",
    "    'product_id',\n",
    "    'location_id',\n",
    "    'gross_quantity',\n",
    "    'date',\n",
    "    'weekday',\n",
    "    'wday',\n",
    "    'month',\n",
    "    'year',\n",
    "    'event_name_1',\n",
    "    'event_type_1',\n",
    "    'event_name_2',\n",
    "    'event_type_2',\n",
    "    'snap_CA',\n",
    "    'snap_TX'\n",
    "    'snap_WI',\n",
    "    'dept_id',\n",
    "    'cat_id',\n",
    "    'state_id',\n",
    "  ]\n",
    "\n",
    "  input_table_specs = [\n",
    "    activities_table_specs,\n",
    "    products_table_specs,\n",
    "    locations_table_specs,\n",
    "  ]\n",
    "\n",
    "  return (\n",
    "    json.dumps(input_table_specs),  # input_table_specs\n",
    "    json.dumps(model_feature_columns),  # model_feature_columns\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835a0f6-9e09-49d1-83d6-ddb360570b15",
   "metadata": {},
   "source": [
    "### Pipeline Component: Get Eval Dataset uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89191440-8184-4edc-8fce-70c674df9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def get_eval_dataset_path_uri(\n",
    "    project: str,\n",
    "    eval_bq_dataset: str,\n",
    "    model_1_table: str,\n",
    "    model_2_table: str,\n",
    ") -> NamedTuple(\n",
    "    'Outputs', \n",
    "    [\n",
    "     ('model_1_bigquery_table_uri', str),\n",
    "     ('model_2_bigquery_table_uri', str),\n",
    "    ]\n",
    "):\n",
    "\n",
    "  # import json\n",
    "\n",
    "  model_1_table_path_name = f'{project}:{eval_bq_dataset}:eval-{model_1_table}'\n",
    "  model_2_table_path_name = f'{project}:{eval_bq_dataset}:eval-{model_2_table}'\n",
    "\n",
    "  print(model_1_table_path_name)\n",
    "  print(model_2_table_path_name)\n",
    "\n",
    "  return (\n",
    "      f'bq://{model_1_table_path_name}',\n",
    "      f'bq://{model_2_table_path_name}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d63ed-853d-457d-8489-e91215e384fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62187121-b476-49eb-8517-141af0749f86",
   "metadata": {},
   "source": [
    "### Pipeline component: combine model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3363e2-ae87-49e7-adac-a592ba26a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.18.0', 'google-cloud-aiplatform==1.7.0'],\n",
    ")\n",
    "def create_combined_preds_table(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  bq_location: str,\n",
    "  model_1_eval_table_uri: str,\n",
    "  model_2_eval_table_uri: str,\n",
    "  model_1_path: Input[Artifact],\n",
    "  model_2_path: Input[Artifact],\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [('combined_preds_table_uri', str)]):\n",
    "  from google.cloud import bigquery\n",
    " \n",
    "  override = bool(override)\n",
    "  bq_client = bigquery.Client(project=project, location=bq_location)\n",
    "  combined_preds_table_name = f'{project}.{dataset}.combined_preds'\n",
    "  \n",
    "  model_1_eval_table_uri=model_1_eval_table_uri\n",
    "  model_2_eval_table_uri=model_2_eval_table_uri\n",
    "\n",
    "  def _sanitize_bq_uri(bq_uri):\n",
    "    if bq_uri.startswith(\"bq://\"):\n",
    "      bq_uri = bq_uri[5:]\n",
    "    return bq_uri.replace(\":\", \".\")\n",
    "\n",
    "  model_1_eval_table_uri = _sanitize_bq_uri(\n",
    "      model_1_eval_table_uri\n",
    "  )\n",
    "\n",
    "  model_2_eval_table_uri = _sanitize_bq_uri(\n",
    "      model_2_eval_table_uri\n",
    "  )\n",
    "\n",
    "  (\n",
    "    bq_client.query(\n",
    "      f\"\"\"\n",
    "      CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'} \n",
    "        `{combined_preds_table_name}`\n",
    "      AS (\n",
    "        SELECT * except(row_number) from\n",
    "          (\n",
    "            SELECT *,ROW_NUMBER() OVER (PARTITION BY datetime,vertex__timeseries__id order by predicted_on_date asc) row_number\n",
    "            FROM\n",
    "            (\n",
    "              SELECT\n",
    "              DATE(table_a.date) as datetime, \n",
    "              DATE(table_a.predicted_on_date) as predicted_on_date,\n",
    "              CAST(table_a.gross_quantity as INTEGER) as gross_quantity, \n",
    "              table_a.vertex__timeseries__id,\n",
    "              table_a.predicted_gross_quantity.value as predicted_gross_quantity_a, \n",
    "              table_b.predicted_gross_quantity.value as predicted_gross_quantity_b\n",
    "              FROM\n",
    "              `{model_1_eval_table_uri}` AS table_a\n",
    "              INNER JOIN `{model_2_eval_table_uri}` AS table_b\n",
    "              ON DATE(table_a.date) = DATE(table_b.date)\n",
    "              and table_a.vertex__timeseries__id = table_b.vertex__timeseries__id\n",
    "              and DATE(table_a.predicted_on_date) = DATE(table_b.predicted_on_date)\n",
    "          ) a\n",
    "          )m\n",
    "          where row_number = 1\n",
    "        );\n",
    "          \"\"\"\n",
    "    )\n",
    "    .result()\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    f'bq://{combined_preds_table_name}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d759ad-b228-48ee-9c49-1890749deb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.18.0'],\n",
    ")\n",
    "def create_final_pred_table(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  bq_location: str,\n",
    "  combined_preds_table_uri: str,\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [('final_preds_table_uri', str)]):\n",
    "  from google.cloud import bigquery\n",
    " \n",
    "  override = bool(override)\n",
    "  bq_client = bigquery.Client(project=project, location=bq_location)\n",
    "  final_preds_table_name = f'{project}.{dataset}.final_preds'\n",
    "  (\n",
    "    bq_client.query(\n",
    "        f\"\"\"\n",
    "      CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'} \n",
    "        `{final_preds_table_name}`\n",
    "        AS (\n",
    "          SELECT\n",
    "              datetime,\n",
    "              vertex__timeseries__id, \n",
    "              gross_quantity as gross_quantity_actual,\n",
    "              ROUND(a.predicted_gross_quantity_a, 2) as model_a_pred,\n",
    "              ROUND(a.predicted_gross_quantity_b, 2) as model_b_pred,\n",
    "              ROUND((a.predicted_gross_quantity_a + a.predicted_gross_quantity_b)/2, 2) AS Final_Pred,\n",
    "              ROUND(ABS(gross_quantity - ((a.predicted_gross_quantity_a + a.predicted_gross_quantity_b)/2)), 2) as Final_Pred_error,\n",
    "          FROM\n",
    "            `{combined_preds_table_uri[5:]}` AS a);\n",
    "            \"\"\"\n",
    "    )\n",
    "    .result()\n",
    "  )\n",
    "\n",
    "  return (\n",
    "      f'bq://{final_preds_table_name}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a845a-5836-4bfd-90d4-4580b3ffec7b",
   "metadata": {},
   "source": [
    "### Pipeline component: Create Forecast input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552fdfa-991e-473b-910d-7b3782f65339",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-aiplatform==1.7.0'],\n",
    ")\n",
    "def create_forecast_input_table_specs(\n",
    "  project: str,\n",
    "  forecast_products_table_uri: str,\n",
    "  forecast_activities_table_uri: str,\n",
    "  forecast_locations_table_uri: str,\n",
    "  forecast_plan_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    ") -> NamedTuple('Outputs', [('forecast_input_table_specs', str)]):\n",
    "  import json\n",
    "  import os\n",
    "\n",
    "  forecast_input_table_specs = [\n",
    "    {\n",
    "      'bigquery_uri': forecast_plan_table_uri,\n",
    "      'table_type': 'FORECASTING_PLAN',\n",
    "    },\n",
    "    {\n",
    "      'bigquery_uri': forecast_activities_table_uri,\n",
    "      'table_type': 'FORECASTING_PRIMARY',\n",
    "      'forecasting_primary_table_metadata': {\n",
    "          'time_column': 'date',\n",
    "          'target_column': 'gross_quantity',\n",
    "          'time_series_identifier_columns': ['product_id', 'location_id'],\n",
    "          'unavailable_at_forecast_columns': [],\n",
    "          'time_granularity': {\n",
    "              'unit': time_granularity_unit,\n",
    "              'quantity': time_granularity_quantity,\n",
    "              },\n",
    "          # 'predefined_splits_column': 'ml_use',\n",
    "          # 'predefined_split_column': 'ml_use', # model_override\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      'bigquery_uri': forecast_products_table_uri,\n",
    "      'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "      'forecasting_attribute_table_metadata': {\n",
    "      'primary_key_column': 'product_id'\n",
    "      }\n",
    "     },\n",
    "    {\n",
    "      'bigquery_uri': forecast_locations_table_uri,\n",
    "      'table_type': 'FORECASTING_ATTRIBUTE',\n",
    "      'forecasting_attribute_table_metadata': {\n",
    "      'primary_key_column': 'location_id'\n",
    "      }\n",
    "    },\n",
    "  ]\n",
    "\n",
    "  print(forecast_input_table_specs)\n",
    "  \n",
    "  return (json.dumps(forecast_input_table_specs),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2d978-679f-4b88-ba9c-d38bb984debf",
   "metadata": {},
   "source": [
    "### Pipeline Component: Get predict table path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aacf84-794e-459a-9189-aa31871f3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def get_predict_table_path(\n",
    "  predict_processed_table: str\n",
    ") -> NamedTuple('Outputs', [('preprocess_bq_uri', str)]):\n",
    "  import json\n",
    "\n",
    "  preprocess_bq_uri = (\n",
    "    json.loads(predict_processed_table)\n",
    "    ['processed_bigquery_table_uri']\n",
    "  )\n",
    "  return (preprocess_bq_uri,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f302d-13b8-4823-b730-716455052e0d",
   "metadata": {},
   "source": [
    "### Pipeline Component: Model_1 Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef737e32-00a7-4af5-b94e-cde9e25fbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b275e-647b-4fd1-ae0a-94559ea01f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-aiplatform==1.7.0'],\n",
    ")\n",
    "def model_1_predict_job_v2(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    eval_bq_dataset: str,\n",
    "    bigquery_source: str,\n",
    "    model_1_path: Input[Artifact],\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('batch_predict_output_bq_uri', str),\n",
    "                            ('batch_predict_job_dict', dict)]):\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  # from google_cloud_pipeline_components.types import artifact_types\n",
    "  import json\n",
    "  import logging\n",
    "\n",
    "  aiplatform.init(\n",
    "      project=project,\n",
    "      location=location,\n",
    "  )\n",
    "  # print(\"model_1_path:\", model_1_path)\n",
    "  # model_aip_uri=model_1_path[\"uri\"]\n",
    "  # print(\"model_1_path[49:]:\", model_1_path[49:])\n",
    "  # model_aip_uri=model_1_path[49:]\n",
    "  # print(\"model_aip_uri_2:\", model_aip_uri)\n",
    "\n",
    "  # model = artifact_types.VertexModel(uri='xxx')\n",
    "\n",
    "  model_resource_path = model_1_path.metadata[\"resourceName\"]\n",
    "  logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "  model = aiplatform.Model(model_name=model_resource_path)\n",
    "  logging.info(\"Model dict:\", model.to_dict())\n",
    "\n",
    "  batch_predict_job = model.batch_predict(\n",
    "      bigquery_source=bigquery_source,\n",
    "      instances_format=\"bigquery\",\n",
    "      bigquery_destination_prefix=f'bq://{project}.{eval_bq_dataset}',\n",
    "      predictions_format=\"bigquery\",\n",
    "      job_display_name='batch-predict-job-1',\n",
    "  )\n",
    "\n",
    "  batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "      batch_predict_job.output_info.bigquery_output_dataset,\n",
    "      batch_predict_job.output_info.bigquery_output_table)\n",
    "  \n",
    "  # if batch_predict_bq_output_uri.startswith(\"bq://\"):\n",
    "  #   batch_predict_bq_output_uri = batch_predict_bq_output_uri[5:]\n",
    "\n",
    "  # batch_predict_bq_output_uri.replace(\":\", \".\")\n",
    "\n",
    "  print(batch_predict_job.to_dict())\n",
    "  return (batch_predict_bq_output_uri, \n",
    "          batch_predict_job.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb749be0-e649-4b57-bce2-1d4e7126bbca",
   "metadata": {},
   "source": [
    "### Pipeline Component: Model_2 Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687b57d-5368-4319-aaf2-ce2c152640c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-aiplatform==1.7.0'],\n",
    ")\n",
    "def model_2_predict_job_v2(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    eval_bq_dataset: str,\n",
    "    bigquery_source: str,\n",
    "    model_2_path: Input[Artifact],\n",
    ") -> NamedTuple('Outputs', [\n",
    "                            ('batch_predict_output_bq_uri', str),\n",
    "                            ('batch_predict_job_dict', dict)]):\n",
    "\n",
    "  from google.cloud import aiplatform\n",
    "  # from google_cloud_pipeline_components.types import artifact_types\n",
    "  import json\n",
    "  import logging\n",
    "\n",
    "  aiplatform.init(\n",
    "      project=project,\n",
    "      location=location,\n",
    "  )\n",
    "  # print(\"model_2_path:\", model_2_path)\n",
    "  # model_aip_uri=model_2_path[\"uri\"]\n",
    "  # print(\"model_2_path[49:]:\", model_2_path[49:])\n",
    "  # model_aip_uri=model_2_path[49:]\n",
    "  # print(\"model_aip_uri_2:\", model_aip_uri)\n",
    "\n",
    "  # model = artifact_types.VertexModel(uri='xxx')\n",
    "\n",
    "  model_resource_path = model_2_path.metadata[\"resourceName\"]\n",
    "  logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "  model = aiplatform.Model(model_name=model_resource_path)\n",
    "  print(\"Model dict:\", model.to_dict())\n",
    "\n",
    "  batch_predict_job = model.batch_predict(\n",
    "      bigquery_source=bigquery_source,\n",
    "      instances_format=\"bigquery\",\n",
    "      bigquery_destination_prefix=f'bq://{project}.{eval_bq_dataset}',\n",
    "      predictions_format=\"bigquery\",\n",
    "      job_display_name='batch-predict-job-2',\n",
    "  )\n",
    "\n",
    "  batch_predict_bq_output_uri = \"{}.{}\".format(\n",
    "      batch_predict_job.output_info.bigquery_output_dataset,\n",
    "      batch_predict_job.output_info.bigquery_output_table)\n",
    "  \n",
    "  # if batch_predict_bq_output_uri.startswith(\"bq://\"):\n",
    "  #   batch_predict_bq_output_uri = batch_predict_bq_output_uri[5:]\n",
    "\n",
    "  # batch_predict_bq_output_uri.replace(\":\", \".\")\n",
    "\n",
    "  print(batch_predict_job.to_dict())\n",
    "  return (batch_predict_bq_output_uri, \n",
    "          batch_predict_job.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c761a-8777-439d-b2f1-d714cf159732",
   "metadata": {},
   "source": [
    "### Pipeline Component: Combine Plan Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea7c49-7763-4178-a4a9-c13b38f5ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(\n",
    "  base_image='python:3.9',\n",
    "  packages_to_install=['google-cloud-bigquery==2.18.0'],\n",
    ")\n",
    "def create_combined_preds_forecast_table(\n",
    "  project: str,\n",
    "  dataset: str,\n",
    "  model_1_pred_table_uri: str,\n",
    "  model_2_pred_table_uri: str,\n",
    "  override: str = 'False',\n",
    ") -> NamedTuple('Outputs', [('combined_preds_forecast_table_uri', str)]):\n",
    "  from google.cloud import bigquery\n",
    " \n",
    "  override = bool(override)\n",
    "  bq_client = bigquery.Client(project=project)\n",
    "  combined_preds_forecast_table_name = f'{project}.{dataset}.combined_preds_forecast'\n",
    "  (\n",
    "    bq_client.query(\n",
    "      f\"\"\"\n",
    "      CREATE {'OR REPLACE TABLE' if override else 'TABLE IF NOT EXISTS'} \n",
    "        `{combined_preds_forecast_table_name}`\n",
    "      AS (SELECT\n",
    "          table_a.date as date, \n",
    "          table_a.vertex__timeseries__id,\n",
    "          ROUND(table_a.predicted_gross_quantity.value,2) as predicted_gross_quantity_a, \n",
    "          ROUND(table_b.predicted_gross_quantity.value, 2) as predicted_gross_quantity_b,\n",
    "          ROUND((table_a.predicted_gross_quantity.value + table_b.predicted_gross_quantity.value)/2, 2) AS Final_Pred\n",
    "          FROM\n",
    "          `{model_1_pred_table_uri[5:]}` AS table_a\n",
    "          INNER JOIN `{model_2_pred_table_uri[5:]}` AS table_b\n",
    "              ON table_a.date = table_b.date\n",
    "              and table_a.vertex__timeseries__id = table_b.vertex__timeseries__id\n",
    "              );\n",
    "          \"\"\"\n",
    "    )\n",
    "    .result()\n",
    "  )\n",
    "\n",
    "  return (\n",
    "    f'bq://{combined_preds_forecast_table_name}',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d949ff-66b4-4c96-b31e-a7b82ea0d99d",
   "metadata": {},
   "source": [
    "## Compile and Run Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdde576-e37c-4845-8f79-76534cf1f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE = 'True' # replace BQ eval tables?\n",
    "VERSION = 'm5_v8' # <--- TODO; Pipeline & model identifier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45385b-1c0d-4380-aa7f-7440d8fc88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TAG = 'train-m5-hier' # <--- TODO; optionally name pipeline\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "  vertex_project: str,\n",
    "  location: str,\n",
    "  version: str,\n",
    "  data_source_dataset: str,\n",
    "  eval_destination_dataset: str,\n",
    "  preprocess_dataset_us: str,\n",
    "  locations_table_uri: str,\n",
    "  forecast_locations_table_uri: str,\n",
    "  products_override: str,\n",
    "  products_table_uri: str,\n",
    "  forecast_products_table_uri: str,\n",
    "  activities_override: str,\n",
    "  activities_table_uri: str,\n",
    "  forecast_activities_table_uri: str,\n",
    "  # activities_expected_historical_last_date: str,\n",
    "  forecast_plan_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    "  context_window: int,\n",
    "  forecast_horizon: int,\n",
    "  override: str,\n",
    "  budget_milli_node_hours: int = 16000,\n",
    "):\n",
    "\n",
    "  create_dataset_op = create_dataset(\n",
    "      project=vertex_project, \n",
    "      vertex_dataset=data_source_dataset, \n",
    "      eval_dataset=eval_destination_dataset, \n",
    "      bq_location=location\n",
    "      )\n",
    "\n",
    "  create_input_table_specs_op = create_input_table_specs(\n",
    "    products_table_uri=products_table_uri,\n",
    "    activities_table_uri=activities_table_uri,\n",
    "    locations_table_uri=locations_table_uri,\n",
    "    time_granularity_unit=time_granularity_unit,\n",
    "    time_granularity_quantity=time_granularity_quantity,\n",
    "  )\n",
    "  create_input_table_specs_op.after(create_dataset_op)\n",
    "\n",
    "  forecasting_validation_op = gcc_aip_forecasting.ForecastingValidationOp(\n",
    "    input_tables=create_input_table_specs_op.outputs['input_table_specs'],\n",
    "    validation_theme='FORECASTING_TRAINING',\n",
    "  )\n",
    "\n",
    "  forecasting_preprocessing_op = gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "    project=vertex_project,\n",
    "    input_tables=create_input_table_specs_op.outputs['input_table_specs'],\n",
    "    preprocessing_bigquery_dataset=data_source_dataset,\n",
    "  )\n",
    "  forecasting_preprocessing_op.after(forecasting_validation_op)\n",
    "  \n",
    "  prepare_data_for_train_op = gcc_aip_forecasting.ForecastingPrepareDataForTrainOp(\n",
    "      input_tables=(\n",
    "          create_input_table_specs_op.outputs['input_table_specs']\n",
    "      ),\n",
    "      preprocess_metadata=(\n",
    "          forecasting_preprocessing_op.outputs['preprocess_metadata']\n",
    "      ),\n",
    "      model_feature_columns=(\n",
    "          create_input_table_specs_op.outputs['model_feature_columns']\n",
    "      ),\n",
    "    )\n",
    "\n",
    "  time_series_dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "    display_name='training_dataset_full_m5', \n",
    "    bq_source=prepare_data_for_train_op.outputs['preprocess_bq_uri'],\n",
    "    project=vertex_project,\n",
    "    location=location,\n",
    "  )\n",
    "\n",
    "  mape_model_version = f'{VERSION}-seq2seq-mape' # TODO: determines model display name and eval BQ table name # f'{VERSION}-l2l-mape'\n",
    "  rmse_model_version = f'{VERSION}-seq2seq-rmse' # TODO: determines model display name and eval BQ table name\n",
    "\n",
    "  get_eval_dataset_path_uri_op = get_eval_dataset_path_uri(\n",
    "      project=vertex_project,\n",
    "      eval_bq_dataset=eval_destination_dataset,\n",
    "      model_1_table=mape_model_version,\n",
    "      model_2_table=rmse_model_version,\n",
    "  )\n",
    "\n",
    "  mape_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "      display_name=f'train-{mape_model_version}',\n",
    "      model_display_name=mape_model_version,\n",
    "      model_labels={'model_override' : 'se2seq-hier'}, # model_override : se2seq-hier, tft\n",
    "      # model_labels={'model_type' : 'l2l'},\n",
    "      dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "      context_window=context_window,\n",
    "      forecast_horizon=forecast_horizon,\n",
    "      budget_milli_node_hours=budget_milli_node_hours,\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      export_evaluated_data_items=True,\n",
    "      export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_1_bigquery_table_uri'], # must be format:``bq://<project_id>:<dataset_id>:<table>``\n",
    "      export_evaluated_data_items_override_destination=True,\n",
    "      target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "      time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "      time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "      time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "      unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "      available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "      data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "      data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "      predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "      column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "      weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "      optimization_objective='minimize-mape',\n",
    "      additional_experiments={\n",
    "          'forecasting_model_type_override': 'seq2seq',\n",
    "          'forecasting_hierarchical_group_column_names':'dept_id, cat_id'},\n",
    "  )\n",
    "\n",
    "  rmse_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "      display_name=f'train-{rmse_model_version}',\n",
    "      model_display_name=rmse_model_version,\n",
    "      model_labels={'model_override' : 'se2seq-hier'}, # model_override : se2seq-hier, tft\n",
    "      # model_labels={'model_type' : 'l2l'},\n",
    "      dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "      context_window=context_window,\n",
    "      forecast_horizon=forecast_horizon,\n",
    "      budget_milli_node_hours=budget_milli_node_hours,\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      export_evaluated_data_items=True,\n",
    "      export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_2_bigquery_table_uri'], # must be format: ``bq://<project_id>:<dataset_id>:<table>``\n",
    "      export_evaluated_data_items_override_destination=True,\n",
    "      target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "      time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "      time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "      time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "      unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "      available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "      data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "      data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "      predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "      column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "      weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "      optimization_objective='minimize-rmse',\n",
    "      additional_experiments={\n",
    "          'forecasting_model_type_override': 'seq2seq',\n",
    "          'forecasting_hierarchical_group_column_names':'dept_id, cat_id'},\n",
    "  )\n",
    "\n",
    "  create_combined_preds_table_op = create_combined_preds_table(\n",
    "      project=vertex_project,\n",
    "      dataset=eval_destination_dataset,\n",
    "      bq_location=location,\n",
    "      model_1_eval_table_uri=get_eval_dataset_path_uri_op.outputs['model_1_bigquery_table_uri'],\n",
    "      model_2_eval_table_uri=get_eval_dataset_path_uri_op.outputs['model_2_bigquery_table_uri'],\n",
    "      model_1_path=mape_model_op.outputs['model'],\n",
    "      model_2_path=rmse_model_op.outputs['model'],\n",
    "  )\n",
    "\n",
    "  create_final_preds_table_op = create_final_pred_table(\n",
    "      project=vertex_project,\n",
    "      dataset=eval_destination_dataset,\n",
    "      bq_location=location,\n",
    "      combined_preds_table_uri=create_combined_preds_table_op.outputs['combined_preds_table_uri'],\n",
    "      override=override,\n",
    "  )\n",
    "\n",
    "  create_forecast_input_table_specs_op = create_forecast_input_table_specs(\n",
    "      project=vertex_project,\n",
    "      forecast_products_table_uri=forecast_products_table_uri,\n",
    "      forecast_activities_table_uri=forecast_activities_table_uri,\n",
    "      forecast_locations_table_uri=forecast_locations_table_uri,\n",
    "      forecast_plan_table_uri=forecast_plan_table_uri,\n",
    "      time_granularity_unit=time_granularity_unit,\n",
    "      time_granularity_quantity=time_granularity_quantity,\n",
    "  )\n",
    "\n",
    "  forecast_validation_op = gcc_aip_forecasting.ForecastingValidationOp(\n",
    "      input_tables=create_forecast_input_table_specs_op.outputs['forecast_input_table_specs'],\n",
    "      validation_theme='FORECASTING_PREDICTION',\n",
    "  )\n",
    "\n",
    "  forecast_preprocess_op = gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "      project=vertex_project,\n",
    "      input_tables=create_forecast_input_table_specs_op.outputs['forecast_input_table_specs'],\n",
    "      preprocessing_bigquery_dataset=preprocess_dataset_us, # TODO: Table needs to be in 'US'\n",
    "  )\n",
    "  forecast_preprocess_op.after(forecast_validation_op)\n",
    "\n",
    "  get_predict_table_path_op = get_predict_table_path(\n",
    "      predict_processed_table=forecast_preprocess_op.outputs['preprocess_metadata'],\n",
    "  )\n",
    "\n",
    "  model_1_predict_job_op = model_1_predict_job_v2(\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      eval_bq_dataset=eval_destination_dataset,\n",
    "      bigquery_source=get_predict_table_path_op.outputs['preprocess_bq_uri'],\n",
    "      model_1_path=mape_model_op.outputs['model'],\n",
    "  )\n",
    "\n",
    "  model_2_predict_job_op = model_2_predict_job_v2(\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      eval_bq_dataset=eval_destination_dataset,\n",
    "      bigquery_source=get_predict_table_path_op.outputs['preprocess_bq_uri'],\n",
    "      model_2_path=rmse_model_op.outputs['model'],\n",
    "  )\n",
    "\n",
    "  create_combined_preds_forecast_table_op = create_combined_preds_forecast_table(\n",
    "      project=vertex_project,\n",
    "      dataset=eval_destination_dataset,\n",
    "      model_1_pred_table_uri=model_1_predict_job_op.outputs['batch_predict_output_bq_uri'],\n",
    "      model_2_pred_table_uri=model_2_predict_job_op.outputs['batch_predict_output_bq_uri'],\n",
    "      override=override,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308bf8e-2621-4a9e-ba97-e20138d5bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='custom_container_pipeline_spec.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221f56b-bbf3-4371-aa4b-025642824a42",
   "metadata": {},
   "source": [
    "Specify pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59e1af-e198-4d80-994d-6d0df2bcc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'sk-ai-ml-poc' # <--- TODO: If not set\n",
    "LOCATION = 'us-central1' # <--- TODO: If not set\n",
    "# SERVICE_ACCOUNT = 'jtotten-project-user@jtotten-project.iam.gserviceaccount.com', # <--- TODO: Change This if needed\n",
    "\n",
    "# BQ dataset for source data source\n",
    "DATA_SOURCE_DATASET = 'm5_forecasting'\n",
    "\n",
    "# BQ dataset for eval tables\n",
    "EVAL_DESTINATION_DATASET = 'm5_eval_forecasting' # <--- TODO: Change this\n",
    "PREPROCESS_DATASET_US = 'm5_preprocessing_us' # <--- TODO: create this in BQ region \"US\"; dont need to create for different pipeline runs\n",
    "\n",
    "# training BQ tables\n",
    "PRODUCTS_TABLE = 'sk-ai-ml-poc.m5_forecasting.products'  #  {type: 'string'} <---TODO: CHANGE THIS\n",
    "LOCATIONS_TABLE = 'sk-ai-ml-poc.m5_forecasting.locations'  #  {type: 'string'} <---TODO: CHANGE THIS\n",
    "ACTIVITIES_TABLE = 'sk-ai-ml-poc.m5_forecasting.activity_all'  #  {type: 'string'} <---TODO: CHANGE THIS\n",
    "# BATCH_PREDICT_TABLE = 'jtotten-project.m5_comp.test_split'  #  {type: 'string'} <---TODO: CHANGE THIS\n",
    "\n",
    "assert PRODUCTS_TABLE, 'the value for this variable must be set'\n",
    "assert LOCATIONS_TABLE, 'the value for this variable must be set'\n",
    "assert ACTIVITIES_TABLE, 'the value for this variable must be set'\n",
    "# assert BATCH_PREDICT_TABLE, 'the value for this variable must be set'\n",
    "\n",
    "# forecast BQ tables\n",
    "FORECAST_PRODUCTS_TABLE = 'sk-ai-ml-poc.m5_forecasting.products'\n",
    "FORECAST_PLAN_TABLE = 'sk-ai-ml-poc.m5_forecasting.plan_table_all'\n",
    "FORECAST_ACTIVITIES_TABLE = 'sk-ai-ml-poc.m5_forecasting.activity_all'\n",
    "FORECAST_LOCATIONS_TABLE = 'sk-ai-ml-poc.m5_forecasting.locations'\n",
    "\n",
    "assert FORECAST_PRODUCTS_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_PLAN_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_ACTIVITIES_TABLE, 'the value for this variable must be set'\n",
    "assert FORECAST_LOCATIONS_TABLE, 'the value for this variable must be set'\n",
    "\n",
    "# TODO: Forecasting Configuration:\n",
    "HISTORY_WINDOW_n = 28 #  {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 28 #  {type: 'integer'} \n",
    "BUDGET_MILLI_NODE_HOURS = 10000\n",
    "\n",
    "# Max date in the activities/sales table\n",
    "# ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE = 'xxxx' #  {type: 'string'}\n",
    "# PREDICTED_ON_DATETIME = 'xxx' # not used for POC, but likely needed for production\n",
    "\n",
    "assert HISTORY_WINDOW_n, 'the value for this variable must be set'\n",
    "assert FORECAST_HORIZON, 'the value for this variable must be set'\n",
    "# assert ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE, 'the value for this variable must be set'\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c960d-ff94-45d4-9e53-6ced9fd7b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True # True creates new pipeline instance for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ffd0f-db36-4abb-b390-03d955594253",
   "metadata": {},
   "source": [
    "Submit pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00be1e8-7c29-4a9e-ac2b-3b62cd69c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PIPELINES.get('train') or overwrite:\n",
    "  response = pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path='custom_container_pipeline_spec.json',\n",
    "    # service_account=SERVICE_ACCOUNT, # <--- TODO: Uncomment if needed\n",
    "    parameter_values={\n",
    "      'vertex_project': PROJECT_ID,\n",
    "      'location': LOCATION,\n",
    "      'version': VERSION,\n",
    "      'data_source_dataset': DATA_SOURCE_DATASET,\n",
    "      'eval_destination_dataset': EVAL_DESTINATION_DATASET,\n",
    "      'preprocess_dataset_us': PREPROCESS_DATASET_US,\n",
    "      'products_override': 'False',\n",
    "      'products_table_uri': f'bq://{PRODUCTS_TABLE}',\n",
    "      'forecast_products_table_uri': f'bq://{FORECAST_PRODUCTS_TABLE}',\n",
    "      'activities_override': 'False',\n",
    "      'activities_table_uri': f'bq://{ACTIVITIES_TABLE}',\n",
    "      'forecast_activities_table_uri': f'bq://{FORECAST_ACTIVITIES_TABLE}',\n",
    "      # 'activities_expected_historical_last_date': ACTIVITIES_EXPECTED_HISTORICAL_LAST_DATE,\n",
    "      # 'predicted_on_date': PREDICTED_ON_DATETIME,\n",
    "      'forecast_plan_table_uri': f'bq://{FORECAST_PLAN_TABLE}',\n",
    "      'locations_table_uri': f'bq://{LOCATIONS_TABLE}',\n",
    "      'forecast_locations_table_uri': f'bq://{FORECAST_LOCATIONS_TABLE}',\n",
    "      'time_granularity_unit': 'DAY',\n",
    "      'time_granularity_quantity': 1,\n",
    "      'context_window': HISTORY_WINDOW_n,\n",
    "      'forecast_horizon': FORECAST_HORIZON,\n",
    "      'override': OVERRIDE,\n",
    "      'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "    },\n",
    "    pipeline_root=f'{GS_PIPELINE_ROOT_PATH}/{VERSION}',\n",
    "  )\n",
    "  PIPELINES['train'] = response['name']\n",
    "  # save_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e36f10-23e0-4741-b369-67819428ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
